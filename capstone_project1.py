# -*- coding: utf-8 -*-
"""Capstone_Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jeN18eGu5kPIQuXdJR67IHukVNhISzvu

**Capstone project 1**

**Phase A: Data Understanding and Preprocessing — The foundation of your premium prediction model for SecureLife Insurance Co.**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# load the dataset
df = pd.read_csv('/content/Insurance Premium Prediction Dataset.csv')

# view the dataset
df

df.head()

df.tail()

df.shape

df.info()

df.describe()

df.isnull().sum()

# Now let us handle the missing values
# Fill the missing the values in the dataset
# This helps us spot columns like Occupation, Health Score, Credit Score, or Previous Claims etc., that might need fixing.

# First, separate numeric and categorical columns
numeric_cols = df.select_dtypes(include=['number']).columns
categorical_cols = df.select_dtypes(exclude=['number']).columns

# Fill numeric columns with their median values
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

# Fill categorical columns with their mode (most frequent value)
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(df)

df.isnull().sum() # Let us recheck for missing values

# Check for duplicate
df.duplicated()

# Convert Policy Start Date to datetime
df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')

# to convert numeric column
df['Annual Income'] = pd.to_numeric(df['Annual Income'], errors='coerce')
df['Credit Score'] = pd.to_numeric(df['Credit Score'], errors='coerce')

df.info()

# Check skewness
df[['Annual Income', 'Health Score', 'Premium Amount']].skew()

# Apply log transformation to reduce skewness
import numpy as np
df['Annual Income'] = np.log1p(df['Annual Income'])
df['Health Score'] = np.log1p(df['Health Score'])
df['Premium Amount'] = np.log1p(df['Premium Amount'])  # Target variable

from datetime import datetime
df['Years Since Policy Start'] = datetime.now().year - df['Policy Start Date'].dt.year

"""**Phase B: Exploratory Data Analysis (EDA)**

"""

## Step 1: Univariate Analysis (One Column at a Time)
# Age distribution
sns.histplot(df['Age'], bins=30, kde=True)
plt.title('Distribution of Age')
plt.show()

# Premium Amount distribution (after log transform)
sns.histplot(df['Premium Amount'], bins=30, kde=True)
plt.title('Distribution of Premium Amount')
plt.show()

# Categorical Columns; Let’s explore how many people are married, single, etc.
# Marital Status count
sns.countplot(data=df, x='Marital Status')
plt.title('Marital Status Distribution')
plt.show()

# Policy Type count
sns.countplot(data=df, x='Policy Type')
plt.title('Policy Type Distribution')
plt.show()

#  Step 2: Bivariate Analysis (Two Columns Together)
# Now we compare features against Premium Amount
sns.scatterplot(data=df, x='Annual Income', y='Premium Amount')
plt.title('Annual Income vs. Premium Amount')
plt.show()

sns.boxplot(data=df, x='Vehicle Age', y='Premium Amount')
plt.title('Vehicle Age vs. Premium Amount')
plt.show()

sns.boxplot(data=df, x='Policy Type', y='Premium Amount')
plt.title('Policy Type vs. Premium Amount')
plt.show()

# Step 3: Multivariate Analysis (Multiple Features Together)
# Correlation Matrix: This shows how numerical features relate to each other and to Premium Amount
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""**Phase C: Feature Engineering**"""

# Step 1: Encode Categorical Variables
from sklearn.preprocessing import OneHotEncoder

# Select categorical columns
cat_cols = ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location',
            'Policy Type', 'Smoking Status', 'Exercise Frequency', 'Property Type']

# Apply OneHotEncoding using pandas for quick conversion
df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# Step 2: Text Preprocessing (Customer feedback)
from textblob import TextBlob

df_encoded['Feedback_Polarity'] = df['Customer Feedback'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
df_encoded['Feedback_Subjectivity'] = df['Customer Feedback'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)

"""**Phase D: Model Development**

"""

# Step 1: Split Dataset into Training and Testing Sets
 from sklearn.model_selection import train_test_split

# Target variable
y = df_encoded['Premium Amount']

# Features — drop unnecessary columns
x = df_encoded.drop(['Premium Amount', 'Policy Start Date', 'Customer Feedback'], axis=1)

# Split into training (80%) and testing (20%)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Step 2: Try Different Regression Model
# LinearRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
x_train_imputed = imputer.fit_transform(x_train)
x_test_imputed = imputer.transform(x_test)


lr_model = LinearRegression()
lr_model.fit(x_train_imputed, y_train)
y_pred_lr = lr_model.predict(x_test_imputed)

# Metrics
print("Linear Regression:")
print("MAE:", mean_absolute_error(y_test, y_pred_lr))
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("R²:", r2_score(y_test, y_pred_lr))

# RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(x_train, y_train)
y_pred_rf = rf_model.predict(x_test)

print("Random Forest:")
print("MAE:", mean_absolute_error(y_test, y_pred_rf))
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("R²:", r2_score(y_test, y_pred_rf))

# GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.impute import SimpleImputer

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
x_train_imputed = imputer.fit_transform(x_train)
x_test_imputed = imputer.transform(x_test)

gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(x_train_imputed, y_train)
y_pred_gb = gb_model.predict(x_test_imputed)

print("Gradient Boosting:")
print("MAE:", mean_absolute_error(y_test, y_pred_gb))
print("MSE:", mean_squared_error(y_test, y_pred_gb))
print("R²:", r2_score(y_test, y_pred_gb))

# Building an Insurance Pricing Model
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.impute import SimpleImputer

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
x_train_imputed = imputer.fit_transform(x_train)


model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.9,
    random_state=42
)

model.fit(x_train_imputed, y_train)

import joblib
import os

# create path
if not os.path.exists('model'):
    os.makedirs('model')

# Save the imputer used for training
joblib.dump(imputer, 'model/imputer.pkl')

# Save the feature columns used for training
joblib.dump(x.columns.tolist(), 'model/training_columns.pkl')

# save for training my model
joblib.dump(model, 'model/Insurance_pricing_model.pkl')

print("imputer.pkl, Insurance_pricing_model.pkl and training_columns.pkl saved successfully.")

"""**Phase E: Model Tuning and Optimization**

"""

# Step 1: Hyperparameter Tuning using GridSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Set hyperparameter options to test
params = {
    'estimator__n_estimators': [100, 200],
    'estimator__learning_rate': [0.05, 0.1],
    'estimator__max_depth': [3, 5],
    'estimator__subsample': [0.8, 1]
}

# Create a pipeline with an imputer and the regressor
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('estimator', GradientBoostingRegressor(random_state=42))
])

# Initialize GridSearchCV
grid_search = GridSearchCV(pipeline, params, cv=3, scoring='r2', n_jobs=-1)

# Fit to the training data
grid_search.fit(x_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Predict on the test set
y_pred_best = best_model.predict(x_test)

# Step 3: Diagnose Overfitting or Underfitting
# Compare training vs testing scores
y_train_pred = best_model.predict(x_train)

print("Train R²:", r2_score(y_train, y_train_pred))
print("Test R²:", r2_score(y_test, y_pred_best))

# Print best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best R² Score (on validation set):", grid_search.best_score_)

"""**Phase F — Interpretation and Insights**

"""

# Step 1: Feature Importance Analysis
# Feature importance from Gradient Boosting
import numpy as np

# Access the actual estimator within the pipeline
feature_importance = pd.Series(best_model.named_steps['estimator'].feature_importances_, index=x.columns)
top_features = feature_importance.sort_values(ascending=False).head(10)

# Plot top features
top_features.plot(kind='barh', figsize=(10,6))
plt.title("Top 10 Features Influencing Insurance Premiums")
plt.xlabel("Feature Importance Score")
plt.gca().invert_yaxis()
plt.show()

# Step 2: Residual Analysis (How Accurate Were Predictions?)
residuals = y_test - y_pred_best
sns.histplot(residuals, bins=30, kde=True)
plt.title("Residual Distribution")
plt.xlabel("Prediction Error")
plt.show()

"""Step 3: Actionable Insights for SecureLife

- Supports Secure Life's vision of tech-driven customer care
- Model is scalable, explainable, and impactful
- Ready to deploy and improve insurance outcomes
- Premiums are most heavily influenced by customer Credit Score, Policy Type, and Health Score — consider focusing risk assessments around these.
- People with higher Vehicle Age and more Previous Claims often receive higher premium estimates — valuable for fraud detection or loyalty discounts.
- Customer Feedback polarity adds context to premium pricing — consider integrating sentiment scoring into your pricing strategy.
- The digital transformation initiative aligns perfectly with these insights — a dynamic pricing system could personalize quotes based on these factors.


Step 4: Business Recommendation

- Use model for dynamic, personalized pricing
- Reward customers with good credit/low-risk profiles
- Expand with external data and deeper NLP methods
- Deploy estimator via secure web interface

git hub link: Topeifeoluwa-2008/Insurance-premium-predictor

streamlit.app:
"""